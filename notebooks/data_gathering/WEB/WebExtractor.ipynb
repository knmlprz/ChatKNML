{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c91e965e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders.recursive_url_loader import RecursiveUrlLoader\n",
    "from newspaper import Article\n",
    "\n",
    "class WebExtractor():\n",
    "    \n",
    "    article = Article(\"\")\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    @staticmethod\n",
    "    def suspected_junk(docs):\n",
    "        \"\"\"\n",
    "        Identifies and returns a list of suspected junk documents based on specific criteria.\n",
    "\n",
    "        :param docs: A list of documents, where each document is represented as a dictionary.\n",
    "                     Each dictionary should have a \"text\" key containing the text content of the document.\n",
    "        :return: A list of suspected junk documents based on the criteria of having less than 300 characters\n",
    "                 or having the same text as another document in the input list.\n",
    "        \"\"\"\n",
    "        junk_docs = []\n",
    "\n",
    "        short_docs = [doc for doc in docs if len(doc[\"text\"]) < 300]\n",
    "        junk_docs.extend(short_docs)\n",
    "\n",
    "        seen_texts = set()\n",
    "        for doc in docs:\n",
    "            if doc[\"text\"] in seen_texts and doc not in junk_docs:\n",
    "                junk_docs.append(doc)\n",
    "            else:\n",
    "                seen_texts.add(doc[\"text\"])\n",
    "        return junk_docs\n",
    "    \n",
    "    @staticmethod\n",
    "    def newspaper_extractor(html):\n",
    "        \"\"\"\n",
    "        Extracts and cleans text content from HTML using the 'newspaper' library.\n",
    "\n",
    "        :param html: HTML content to be processed.\n",
    "        :return: Cleaned and concatenated text extracted from the HTML.\n",
    "        \"\"\"\n",
    "        WebExtractor.article.set_html(html)\n",
    "        WebExtractor.article.parse()\n",
    "        return ' '.join(WebExtractor.article.text.split())\n",
    "    \n",
    "    @staticmethod\n",
    "    def data_structure_converter(docs):\n",
    "        \"\"\"\n",
    "        Converts a list of documents into a specific data structure.\n",
    "\n",
    "        :param docs: List of documents to be converted.\n",
    "        :return: List of dictionaries, each representing a document with 'text' and 'url' keys.\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        for doc in docs:\n",
    "            documents.append({'text': doc.page_content,\n",
    "                             'url': doc.metadata['source']})\n",
    "        return documents\n",
    "    \n",
    "    @staticmethod\n",
    "    def extract_web(url, depth=1):\n",
    "        \"\"\"\n",
    "        Extracts web content from the specified URL(s) using a recursive approach.\n",
    "\n",
    "        :param url: URL or list of URLs to extract content from.\n",
    "        :param depth: Maximum depth for recursive extraction (default is 1).\n",
    "        :return: Tuple containing two lists of dictionaries - (correct_documents, junk_documents).\n",
    "                 Each dictionary represents a document with 'text' and 'url' keys.\n",
    "        :raises TypeError: If the 'url' parameter is not of the string type or a list of string variables.\n",
    "        \"\"\"\n",
    "        if not (isinstance(url, str) or (isinstance(url, list) and all(isinstance(item, str) for item in url))):\n",
    "            raise TypeError(\"The 'url' parameter should be of the string type or a list with string variables.\")\n",
    "    \n",
    "        if isinstance(url, str):\n",
    "            url_list = [url]\n",
    "        else:\n",
    "            url_list = url\n",
    "        \n",
    "        all_docs = []\n",
    "        for address in url_list:\n",
    "            try:\n",
    "                loader = RecursiveUrlLoader(url=address, max_depth=depth, extractor=WebExtractor.newspaper_extractor)\n",
    "                all_docs.extend(loader.load())\n",
    "            except Exception as e:\n",
    "                print(f\"Exception: {e}\")\n",
    "                break\n",
    "        \n",
    "        documents = WebExtractor.data_structure_converter(all_docs)\n",
    "        junk_documents = WebExtractor.suspected_junk(documents)\n",
    "        \n",
    "        correct_documents = [doc for doc in documents if doc not in junk_documents]\n",
    "        \n",
    "        print(f\"Number of pages downloaded: {len(all_docs)}\")\n",
    "        print(f\"Number of suspected junks: {len(junk_documents)}\")\n",
    "        \n",
    "        return correct_documents, junk_documents"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
